{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "#from gym.wrappers.record_video import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "'''\n",
    "Bunch of Hyper parameters (Which you might have to tune later)\n",
    "'''\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "\n",
    "GAMMA = 0.99            # discount factor\n",
    "LR = 5e-4               # learning rate\n",
    "\n",
    "\n",
    "\n",
    "class QNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self,fc1_units, fc2_units,type,state_size, action_size):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork1, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.advantage_fc = nn.Linear(fc2_units, action_size)\n",
    "        self.value_fc = nn.Linear(fc2_units, 1)\n",
    "        self.type = type\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        advantage = self.advantage_fc(x)\n",
    "        value = self.value_fc(x)\n",
    "        if self.type == 1:\n",
    "            q_values = value + (advantage - advantage.mean(dim = 1, keepdim = True))\n",
    "        else:\n",
    "            q_values = value + (advantage - torch.max(advantage))\n",
    "        return q_values\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class TutorialAgent():\n",
    "    def __init__(self,lr,Batch_Size,update_rate,type,state_size, action_size,fc1 =128, fc2= 64, seed = 0):\n",
    "        ''' Agent Environment Interaction '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = Batch_Size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.qnetwork_local = QNetwork1(int(fc1),int(fc2),type,state_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork1(int(fc1),int(fc2),type,state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, int(Batch_Size), seed)\n",
    "        self.t_step = 0\n",
    "        self.update_rate = int(update_rate)\n",
    "        \n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "        self.t_step = (self.t_step + 1) % self.update_rate\n",
    "        if self.t_step == 0:\n",
    "\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state, policy, hyp=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        if(policy == \"eps greedy\"):\n",
    "          if random.random() > hyp:\n",
    "              return np.argmax(action_values.cpu().data.numpy())\n",
    "          else:\n",
    "              return random.choice(np.arange(self.action_size))\n",
    "        if(policy == \"softmax\"):\n",
    "          action_probs = softmax(action_values.cpu().data.numpy().flatten() / hyp)\n",
    "          return np.random.choice(np.arange(self.action_size), p=action_probs)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    ''' Trial run to check if algorithm runs and saves the data '''\n",
    "\n",
    "class DuelingDQN:\n",
    "    def __init__(self,env, policy, agent):\n",
    "        self.env = env\n",
    "        self.episodes = 4000\n",
    "        self.max_t = 10000\n",
    "        self.policy = policy\n",
    "        self.agent = agent\n",
    "\n",
    "    def train(self):\n",
    "        hyp_start = 1.0\n",
    "        hyp_end = 0.01\n",
    "        hyp_decay = 0.995\n",
    "        hyp = hyp_start\n",
    "        scores_window = deque(maxlen=100)\n",
    "        avg_rewards = []\n",
    "        for i_episode in range(1, self.episodes+1):\n",
    "            state,_ = self.env.reset()\n",
    "            score = 0\n",
    "            for t in range(self.max_t):\n",
    "                action = self.agent.act(state, self.policy, hyp = hyp)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.agent.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                 break\n",
    "            scores_window.append(score)\n",
    "            hyp = max(hyp_end, hyp_decay*hyp)\n",
    "            ''' decrease epsilon '''\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "            avg_rewards.append(np.mean(scores_window))\n",
    "            if i_episode % 10 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            #if np.mean(scores_window)>= self.env.spec.reward_threshold or i_episode >350:\n",
    "            if i_episode >350:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "                break\n",
    "        return avg_rewards\n",
    "    \n",
    "    def regret(self,rewards):\n",
    "        regret = 0\n",
    "        for i in range(rewards):\n",
    "            regret += self.env.spec.reward_threshold - rewards[i]\n",
    "        return regret\n",
    "\n",
    "    def PerformExpmt(self,num_expmt,type):\n",
    "        reward_avgs = []\n",
    "        for i in range(num_expmt):  \n",
    "            print(\"Experiment: %d\"%(i+1))\n",
    "            rewards = self.train()   \n",
    "            reward_avgs.append(np.asarray(rewards))\n",
    "        reward_avgs_mean = np.mean(np.array(reward_avgs), axis=0)\n",
    "        reward_avgs_std = np.std(reward_avgs, axis=0)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(self.episodes), reward_avgs_mean, label='Reward Avg', color='blue')\n",
    "        plt.fill_between(range(self.episodes), reward_avgs_mean - reward_avgs_std, reward_avgs_mean + reward_avgs_std, alpha=0.3, color='blue')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{type}.png\")\n",
    "        plt.show()\n",
    "        return reward_avgs_mean, reward_avgs_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "type = 2 #max\n",
    "lr = 0.004891  \n",
    "batch = 64\n",
    "update = 20   \n",
    "for i in range(2):\n",
    "    agent = TutorialAgent(lr,batch,update,type,state_size,action_size, seed = 0)\n",
    "    DDQN = DuelingDQN(env,\"softmax\",agent)\n",
    "    DDQN.PerformExpmt(2,i)  # type 0 is max and type 1 is mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Expmts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDQN.PerformExpmt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Objective(lr,batch,update):\n",
    "    env = gym.make(\"Acrobot-v1\")\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    type = 1 #mean\n",
    "    # type  = 2 # max\n",
    "    agent = TutorialAgent(lr,batch,update,type,state_size,action_size, seed = 0)\n",
    "    DDQN = DuelingDQN(env,\"softmax\",agent)\n",
    "    avg_rewards = DDQN.train()\n",
    "    return avg_rewards\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "        },\n",
    "    'lr': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.00001,\n",
    "        'max': 0.01\n",
    "      },\n",
    "    'batch': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 40,\n",
    "        'max': 80\n",
    "      },\n",
    "    'update': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 20,\n",
    "        'max': 30\n",
    "      }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")\n",
    "\n",
    "def train(config=sweep_config):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        avg_reward = Objective(config.lr,config.batch,config.update)\n",
    "\n",
    "\n",
    "        for epoch in range(len(avg_reward)):\n",
    "            wandb.log({\"Score\": avg_reward[epoch], \"epoch\": epoch})   \n",
    "\n",
    "wandb.agent(sweep_id, train, count=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
