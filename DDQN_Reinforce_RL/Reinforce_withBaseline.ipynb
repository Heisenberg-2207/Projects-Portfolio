{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import wandb\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set constants for training\n",
    "seed = 543 #constant\n",
    "gamma = 0.99 #constant\n",
    "lr_optim = 1e-3 #tunable\n",
    "lr_optimV = 1e-3 #tunable\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,hidden_size,state_shape = 4,action_size = 2 ):\n",
    "        super(Network, self).__init__()\n",
    "        self.affine1 = nn.Linear(state_shape, int(hidden_size))\n",
    "        self.action_head = nn.Linear(int(hidden_size), action_size)\n",
    "        self.saved_actions = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_states = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "        return action_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNetwork(nn.Module):\n",
    "    def __init__(self, state_size, seed, fc1_units=128, fc2_units=64):\n",
    "        super(VNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_MCWB:\n",
    "\n",
    "    def __init__(self,hidden,lp,lv):\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        self.env = env\n",
    "        seed  =543\n",
    "        self.episodes = 500\n",
    "        env.reset()\n",
    "        state_shape = env.observation_space.shape[0]\n",
    "        no_of_actions = env.action_space.n\n",
    "        policy = Network(hidden, state_shape,no_of_actions).to(device)\n",
    "        self.policy = policy\n",
    "        self.optimizerP = optim.Adam(policy.parameters(), lp)\n",
    "        self.vnetwork_local = VNetwork(state_shape, seed).to(device)\n",
    "        self.optimizerV = optim.Adam(self.vnetwork_local.parameters(), lv)\n",
    "        self.vnetwork_target = VNetwork(state_shape, seed).to(device)\n",
    "        self.max_len = 10000\n",
    "\n",
    "    def learn_Value(self, states, actions, rewards, next_states, dones):\n",
    "        next_states = torch.tensor(next_states).to(device)\n",
    "        V_targets_next = self.vnetwork_target(next_states).detach()\n",
    "        V_targets =  + (gamma * V_targets_next * (1 - dones))\n",
    "        actions = torch.tensor(actions).view(-1, 1).to(device)\n",
    "        V_expected = self.vnetwork_local(torch.tensor(states).to(device))\n",
    "        loss = F.mse_loss(V_expected, V_targets)\n",
    "        self.optimizerV.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.vnetwork_local.parameters():\n",
    "              param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizerV.step()\n",
    "        \n",
    "    def select_action(self,state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        probs = self.policy(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self,rewards, states, actions):\n",
    "        G = 0\n",
    "        gamma = 0.99\n",
    "        self.optimizerP.zero_grad()\n",
    "        for i in reversed(range(len(rewards))):  \n",
    "            reward = rewards[i]\n",
    "            state = torch.tensor(states[i].reshape(1, -1),\n",
    "                                 dtype=torch.float).to(device)\n",
    "            action = torch.tensor(actions[i]).view(-1, 1).to(device)\n",
    "            log_prob = torch.log(self.policy(state)).gather(1,action)\n",
    "            #print(log_prob)\n",
    "            G = gamma * G + reward\n",
    "            advantage =  G  - self.vnetwork_local(state)\n",
    "            #print(advantage)\n",
    "            loss = -log_prob * advantage \n",
    "            loss.backward()             \n",
    "        self.optimizerP.step()\n",
    "        del self.policy.episode_rewards[:]\n",
    "        del self.policy.saved_actions[:]\n",
    "        del self.policy.episode_states[:]\n",
    "     \n",
    "    def train(self):\n",
    "        total_reward = []\n",
    "        avg_reward = []\n",
    "        running_reward = 10\n",
    "        # run infinitely many episodes\n",
    "        for i_episode in range(self.episodes):\n",
    "            # reset environment and episode reward\n",
    "            state, _ = self.env.reset()\n",
    "            ep_reward = 0\n",
    "            for t in range(1, self.max_len):\n",
    "            # select action from policy\n",
    "                self.policy.episode_states.append(state)\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.learn_Value(state, action, reward, next_state, done)\n",
    "                self.policy.episode_rewards.append(reward)\n",
    "                self.policy.saved_actions.append(action)\n",
    "                state = next_state\n",
    "                ep_reward += reward\n",
    "                if done:\n",
    "                   break \n",
    "            total_reward.append(ep_reward)\n",
    "            self.update(self.policy.episode_rewards, self.policy.episode_states, self.policy.saved_actions)   \n",
    "            running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "            avg_reward.append(running_reward)\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "            \"\"\" if np.mean(avg_reward)>= self.env.spec.reward_threshold or i_episode >400: \"\"\"\n",
    "            if i_episode >400:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(avg_reward)))\n",
    "                break\n",
    "        return avg_reward\n",
    "    \n",
    "    def PerformExpmt(self,num_expmt):\n",
    "        reward_avgs = []\n",
    "        for i in range(num_expmt):  \n",
    "            print(\"Experiment: %d\"%(i+1))\n",
    "            rewards = self.train()   \n",
    "            reward_avgs.append(np.asarray(rewards))\n",
    "        reward_avgs_mean = np.mean(np.array(reward_avgs), axis=0)\n",
    "        reward_avgs_std = np.std(reward_avgs, axis=0)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(self.episodes), reward_avgs_mean, label='Reward Avg', color='blue')\n",
    "        plt.fill_between(range(self.episodes), reward_avgs_mean - reward_avgs_std, reward_avgs_mean + reward_avgs_std, alpha=0.3, color='blue')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.legend()\n",
    "        plt.savefig('withBaseline.png')\n",
    "        plt.show()\n",
    "        return reward_avgs_mean, reward_avgs_std\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Running with tuned params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = 0.0004492\n",
    "lv = 0.000682\n",
    "hidden = 256\n",
    "reinforce = REINFORCE_MCWB(hidden,lp,lv)\n",
    "avg_rewards=reinforce.train()\n",
    "plt.plot(avg_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(hidden,lp,lv):\n",
    "    reinforce = REINFORCE_MCWB(hidden,lp,lv)\n",
    "    avg_reward = reinforce.train()\n",
    "    return avg_reward\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "        },\n",
    "    'hidden': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        \"values\": [64, 128, 256],\n",
    "      },\n",
    "      'lp': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.00001,\n",
    "        'max': 0.001\n",
    "      },\n",
    "      'lv': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.00001,\n",
    "        'max': 0.001\n",
    "      }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")\n",
    "\n",
    "def tuner(config=sweep_config):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        avg_reward = tune(config.hidden,config.lp,config.lv)\n",
    "        for epoch in range(len(avg_reward)):\n",
    "            wandb.log({\"Score\": avg_reward[epoch], \"epoch\": epoch})   \n",
    "\n",
    "wandb.agent(sweep_id, tuner, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
