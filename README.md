## Projects

<img align="left" width="250" height="150" src="https://github.com/Shubham2376G/AI_Projects/blob/main/Images/data_vizz.png"> [Localisation and Mapping]

I developed an image recognition system for detecting, localizing, and mapping wheelchairs as part of an automatic docking solution for a wheelchair power assist. Utilizing a deep learning model, I annotated over 300 images to create a robust training dataset, achieving an accuracy of 86%. I also initiated the design of a scaled-down prototype to implement path-planning algorithms, laying the groundwork for the complete power assist system.

#

<img align="left" width="250" height="150" src="https://github.com/Shubham2376G/AI_Projects/blob/main/Images/amex-card1708.jpg"> [Discrete Fourier Transformation]

In the Parallel Scientific Computing course, I implemented the Discrete Fourier Transform (DFT) from scratch, utilizing OpenMP for shared memory and MPI for distributed memory in C and C++. I assessed the performance of these parallel implementations by analyzing scalability, efficiency, and speedup, demonstrating a deep understanding of parallel computing techniques and their practical applications.

#

<img align="left" width="250" height="150" src="https://github.com/Shubham2376G/AI_Projects/blob/main/Images/llm_finetuning.png"> [QLearning and SARSA RL]

The work involved hyperparameter tuning using Bayesian Optimization to maximize the reward function in various gridworld environments. The experiments compared the performance of the two algorithms across different world configurations, analyzing factors such as reward, number of steps to reach the goal, and state visitation frequency. The study provides insights into the efficiency and effectiveness of the softmax and epsilon-greedy policies under different conditions.

#

<img align="left" width="250" height="150" src="https://github.com/Shubham2376G/AI_Projects/blob/main/Images/falcon.jfif"> [DDQN and Reinforce RL]

The project explores the Dueling Deep Q-Network (DQN) and Monte Carlo (MC) Reinforce methods with and without baselines, applied to environments like Acrobot-v1 and CartPole-v1. The report includes sections on hyperparameter tuning, code implementation, and the use of Weights and Biases (WANDB) for optimization. It concludes with an analysis of the results, discussing the performance and convergence of the algorithms under various configurations.

#

<img align="left" width="250" height="150" src="https://github.com/Shubham2376G/AI_Projects/blob/main/Images/tennis.jpg"> [SMDP and IntraOption RL]

The work investigates various methods for learning and optimizing policies using these techniques, including cases with fixed policy options, learnable options, and pseudo rewards. The project includes code implementations, parameter tuning, and performance analysis through plots and Q-tables. The project aims to develop an understanding of how different Q-learning approaches perform under various conditions and to compare the results of SMDP Q-learning and Intra-Option Q-learning.
