## Projects

<img align="left" width="250" height="150" src="https://github.com/Heisenberg-2207/Projects-Portfolio/blob/main/Localisation%20and%20Mapping/Result.png"> *[Localisation and Mapping](https://github.com/Heisenberg-2207/Projects-Portfolio/tree/main/Localisation%20and%20Mapping)*

Developed an image recognition system for detecting, localizing, and mapping wheelchairs as part of an automatic docking solution for a wheelchair power assist. Utilizing a deep learning model, annotated over 300 images to create a robust training dataset, achieving an accuracy of 86%.

#

<img align="left" width="250" height="150" src="https://github.com/Heisenberg-2207/Projects-Portfolio/blob/main/Parallel%20Discrete%20Fourier%20Transformation/Result.png"> *[Discrete Fourier Transformation](https://github.com/Heisenberg-2207/Projects-Portfolio/tree/main/Parallel%20Discrete%20Fourier%20Transformation)*

Implemented the Discrete Fourier Transform (DFT) from scratch, utilizing OpenMP for shared memory and MPI for distributed memory in C and C++. Assessed the performance of these parallel implementations by analyzing scalability, efficiency, and speedup.

#

<img align="left" width="250" height="150" src="https://github.com/Heisenberg-2207/Projects-Portfolio/blob/main/QLearning_SARSA_RL/world_img.png"> *[QLearning and SARSA RL](https://github.com/Heisenberg-2207/Projects-Portfolio/tree/main/QLearning_SARSA_RL)*

Hyperparameter tuning using Bayesian Optimization to maximize the reward function. Compared the performance of the two algorithms across different world configurations, analyzing factors such as reward, number of steps to reach the goal, and state visitation frequency. The study provides insights into the efficiency and effectiveness of the softmax and epsilon-greedy policies under different conditions.

#

<img align="left" width="250" height="150" src="https://github.com/Heisenberg-2207/Projects-Portfolio/blob/main/DDQN_Reinforce_RL/Result.png"> *[DDQN and Reinforce RL](https://github.com/Heisenberg-2207/Projects-Portfolio/tree/main/DDQN_Reinforce_RL)*

Dueling Deep Q-Network (DQN) and Monte Carlo (MC) Reinforce methods with and without baselines, applied to environments like Acrobot-v1 and CartPole-v1. The report includes sections on hyperparameter tuning, code implementation, and the use of Weights and Biases (WANDB) for optimization. Analysis of the results, discussing the performance and convergence of the algorithms under various configurations.

#

<img align="left" width="250" height="150" src="https://github.com/Heisenberg-2207/Projects-Portfolio/blob/main/SMDP_IntraOption_RL/Result.png"> *[SMDP and IntraOption RL](https://github.com/Heisenberg-2207/Projects-Portfolio/tree/main/SMDP_IntraOption_RL)*

The work investigates various methods for learning and optimizing policies using these techniques, including cases with fixed policy options, learnable options, and pseudo rewards. The project includes code implementations, parameter tuning, and performance analysis through plots and Q-tables. The project aims to develop an understanding of how different Q-learning approaches perform under various conditions
